{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR100_keras_Vgg16",
      "provenance": [],
      "authorship_tag": "ABX9TyMeMlnaIu9CuerF/m3KRc/p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sruthi1996/DeepLearning/blob/master/CIFAR100_keras_Vgg16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxO1wT89DOH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar100\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m5y9iedDRoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M723hNf6DV07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "afab9b63-be5c-481e-d86d-a16e747d5994"
      },
      "source": [
        "class cifar100vgg:\n",
        "    \n",
        "    def __init__(self,train=True):\n",
        "        self.num_classes = 100\n",
        "        self.weight_decay = 0.0001\n",
        "        self.x_shape = [32,32,3]\n",
        "\n",
        "        self.model = self.build_model()\n",
        "        if train:\n",
        "            self.model = self.train(self.model)\n",
        "        else:\n",
        "            self.model.load_weights('cifar100vgg.h5')\n",
        "   \n",
        "    def build_model(self):\n",
        "        # Building vgg n/w with dropout and weight decay in each layer using sequential model.\n",
        "\n",
        "        model = Sequential()\n",
        "        weight_decay = self.weight_decay\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), padding='same',\n",
        "                         input_shape=self.x_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(self.num_classes))\n",
        "        model.add(Activation('softmax'))\n",
        "        return model\n",
        "\n",
        "    #normalize the inputs with least mean and variance; when input and test dataset are given; normalised input and test dataset are generated.\n",
        "    def normalize(self,X_train,X_test):\n",
        "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
        "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "        print(mean)\n",
        "        print(std)\n",
        "        X_train = (X_train-mean)/(std+1e-7)\n",
        "        X_test = (X_test-mean)/(std+1e-7)\n",
        "        return X_train, X_test\n",
        "\n",
        "    def normalize_production(self,x):\n",
        "        mean = 121.936\n",
        "        std = 68.389\n",
        "        return (x-mean)/(std+1e-7)\n",
        "\n",
        "    def predict(self,x,normalize=True,batch_size=50):\n",
        "        if normalize:\n",
        "            x = self.normalize_production(x)\n",
        "        return self.model.predict(x,batch_size)\n",
        "\n",
        "    def train(self,model):\n",
        "\n",
        "        #training parameters\n",
        "        batch_size = 128\n",
        "        maxepoches = 50\n",
        "        learning_rate = 0.2\n",
        "        lr_decay = 1e-6\n",
        "        lr_drop = 15\n",
        "\n",
        "        # The data, shuffled and split between train and test sets:\n",
        "        (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train, x_test = self.normalize(x_train, x_test)\n",
        "\n",
        "        y_train = keras.utils.to_categorical(y_train, self.num_classes)\n",
        "        y_test = keras.utils.to_categorical(y_test, self.num_classes)\n",
        "\n",
        "\n",
        "        def lr_scheduler(epoch):\n",
        "            return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "        reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "\n",
        "        #data augmentation\n",
        "        datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  \n",
        "            samplewise_center=False, \n",
        "            featurewise_std_normalization=False,\n",
        "            samplewise_std_normalization=False,\n",
        "            zca_whitening=False,\n",
        "            rotation_range=15,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            horizontal_flip=True,\n",
        "            vertical_flip=False)\n",
        "        \n",
        "        datagen.fit(x_train)\n",
        "\n",
        "\n",
        "\n",
        "        #optimization\n",
        "        sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
        "\n",
        "\n",
        "        # Learning rate will drop every 25 epoches; training for total 50 epochs\n",
        "\n",
        "        historytemp = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                         batch_size=batch_size),\n",
        "                            steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                            epochs=maxepoches,\n",
        "                            validation_data=(x_test, y_test),callbacks=[reduce_lr],verbose=2)\n",
        "        model.save_weights('cifar100vgg.h5')\n",
        "        return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "\n",
        "    y_train = keras.utils.to_categorical(y_train, 100)\n",
        "    y_test = keras.utils.to_categorical(y_test, 100)\n",
        "\n",
        "    model = cifar100vgg()\n",
        "\n",
        "    predicted_x = model.predict(x_test)\n",
        "    residuals = (np.argmax(predicted_x,1)!=np.argmax(y_test,1))\n",
        "    loss = sum(residuals)/len(residuals)\n",
        "    print(\"Full validation loss: \",loss)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "121.93584\n",
            "68.38902\n",
            "Epoch 1/50\n",
            " - 44s - loss: 16.6241 - accuracy: 0.0196 - val_loss: 15.8339 - val_accuracy: 0.0155\n",
            "Epoch 2/50\n",
            " - 40s - loss: 13.3306 - accuracy: 0.0339 - val_loss: 12.3956 - val_accuracy: 0.0157\n",
            "Epoch 3/50\n",
            " - 40s - loss: 10.8090 - accuracy: 0.0440 - val_loss: 10.0051 - val_accuracy: 0.0331\n",
            "Epoch 4/50\n",
            " - 40s - loss: 8.9829 - accuracy: 0.0516 - val_loss: 8.4020 - val_accuracy: 0.0414\n",
            "Epoch 5/50\n",
            " - 40s - loss: 7.6423 - accuracy: 0.0598 - val_loss: 7.2371 - val_accuracy: 0.0462\n",
            "Epoch 6/50\n",
            " - 40s - loss: 6.6491 - accuracy: 0.0685 - val_loss: 6.2388 - val_accuracy: 0.0739\n",
            "Epoch 7/50\n",
            " - 40s - loss: 5.9162 - accuracy: 0.0800 - val_loss: 6.0250 - val_accuracy: 0.0552\n",
            "Epoch 8/50\n",
            " - 40s - loss: 5.3688 - accuracy: 0.0894 - val_loss: 5.1480 - val_accuracy: 0.0858\n",
            "Epoch 9/50\n",
            " - 40s - loss: 4.9238 - accuracy: 0.1039 - val_loss: 4.6604 - val_accuracy: 0.1236\n",
            "Epoch 10/50\n",
            " - 40s - loss: 4.5682 - accuracy: 0.1224 - val_loss: 4.3281 - val_accuracy: 0.1533\n",
            "Epoch 11/50\n",
            " - 40s - loss: 4.3169 - accuracy: 0.1426 - val_loss: 4.1542 - val_accuracy: 0.1649\n",
            "Epoch 12/50\n",
            " - 40s - loss: 4.1149 - accuracy: 0.1612 - val_loss: 3.9737 - val_accuracy: 0.1823\n",
            "Epoch 13/50\n",
            " - 40s - loss: 3.9488 - accuracy: 0.1834 - val_loss: 3.7229 - val_accuracy: 0.2238\n",
            "Epoch 14/50\n",
            " - 40s - loss: 3.7810 - accuracy: 0.2097 - val_loss: 3.4707 - val_accuracy: 0.2672\n",
            "Epoch 15/50\n",
            " - 40s - loss: 3.6635 - accuracy: 0.2331 - val_loss: 3.6129 - val_accuracy: 0.2461\n",
            "Epoch 16/50\n",
            " - 39s - loss: 3.3805 - accuracy: 0.2859 - val_loss: 3.2123 - val_accuracy: 0.3193\n",
            "Epoch 17/50\n",
            " - 39s - loss: 3.2442 - accuracy: 0.3086 - val_loss: 3.0449 - val_accuracy: 0.3472\n",
            "Epoch 18/50\n",
            " - 39s - loss: 3.1556 - accuracy: 0.3210 - val_loss: 2.9508 - val_accuracy: 0.3692\n",
            "Epoch 19/50\n",
            " - 39s - loss: 3.1073 - accuracy: 0.3349 - val_loss: 3.0189 - val_accuracy: 0.3728\n",
            "Epoch 20/50\n",
            " - 39s - loss: 3.0543 - accuracy: 0.3491 - val_loss: 3.0059 - val_accuracy: 0.3705\n",
            "Epoch 21/50\n",
            " - 39s - loss: 3.0228 - accuracy: 0.3568 - val_loss: 2.8294 - val_accuracy: 0.4026\n",
            "Epoch 22/50\n",
            " - 39s - loss: 2.9822 - accuracy: 0.3722 - val_loss: 2.8218 - val_accuracy: 0.4089\n",
            "Epoch 23/50\n",
            " - 39s - loss: 2.9459 - accuracy: 0.3835 - val_loss: 2.7504 - val_accuracy: 0.4303\n",
            "Epoch 24/50\n",
            " - 39s - loss: 2.9225 - accuracy: 0.3946 - val_loss: 2.7436 - val_accuracy: 0.4381\n",
            "Epoch 25/50\n",
            " - 39s - loss: 2.9099 - accuracy: 0.4051 - val_loss: 2.9027 - val_accuracy: 0.4282\n",
            "Epoch 26/50\n",
            " - 39s - loss: 2.8593 - accuracy: 0.4202 - val_loss: 2.7777 - val_accuracy: 0.4391\n",
            "Epoch 27/50\n",
            " - 39s - loss: 2.8620 - accuracy: 0.4247 - val_loss: 2.8848 - val_accuracy: 0.4383\n",
            "Epoch 28/50\n",
            " - 39s - loss: 2.8511 - accuracy: 0.4312 - val_loss: 2.7449 - val_accuracy: 0.4612\n",
            "Epoch 29/50\n",
            " - 39s - loss: 2.8377 - accuracy: 0.4409 - val_loss: 2.8415 - val_accuracy: 0.4549\n",
            "Epoch 30/50\n",
            " - 39s - loss: 2.8247 - accuracy: 0.4494 - val_loss: 2.7131 - val_accuracy: 0.4810\n",
            "Epoch 31/50\n",
            " - 39s - loss: 2.6303 - accuracy: 0.4929 - val_loss: 2.5141 - val_accuracy: 0.5253\n",
            "Epoch 32/50\n",
            " - 39s - loss: 2.5332 - accuracy: 0.5108 - val_loss: 2.5681 - val_accuracy: 0.5153\n",
            "Epoch 33/50\n",
            " - 39s - loss: 2.4934 - accuracy: 0.5169 - val_loss: 2.5228 - val_accuracy: 0.5275\n",
            "Epoch 34/50\n",
            " - 40s - loss: 2.4534 - accuracy: 0.5266 - val_loss: 2.5145 - val_accuracy: 0.5226\n",
            "Epoch 35/50\n",
            " - 40s - loss: 2.4390 - accuracy: 0.5275 - val_loss: 2.3291 - val_accuracy: 0.5584\n",
            "Epoch 36/50\n",
            " - 40s - loss: 2.4255 - accuracy: 0.5309 - val_loss: 2.4962 - val_accuracy: 0.5266\n",
            "Epoch 37/50\n",
            " - 39s - loss: 2.4037 - accuracy: 0.5381 - val_loss: 2.5162 - val_accuracy: 0.5288\n",
            "Epoch 38/50\n",
            " - 40s - loss: 2.3943 - accuracy: 0.5395 - val_loss: 2.4865 - val_accuracy: 0.5305\n",
            "Epoch 39/50\n",
            " - 39s - loss: 2.3762 - accuracy: 0.5449 - val_loss: 2.3409 - val_accuracy: 0.5642\n",
            "Epoch 40/50\n",
            " - 39s - loss: 2.3570 - accuracy: 0.5518 - val_loss: 2.6543 - val_accuracy: 0.5176\n",
            "Epoch 41/50\n",
            " - 39s - loss: 2.3656 - accuracy: 0.5482 - val_loss: 2.3915 - val_accuracy: 0.5578\n",
            "Epoch 42/50\n",
            " - 39s - loss: 2.3481 - accuracy: 0.5563 - val_loss: 2.3948 - val_accuracy: 0.5563\n",
            "Epoch 43/50\n",
            " - 39s - loss: 2.3535 - accuracy: 0.5581 - val_loss: 2.4105 - val_accuracy: 0.5601\n",
            "Epoch 44/50\n",
            " - 39s - loss: 2.3323 - accuracy: 0.5617 - val_loss: 2.3794 - val_accuracy: 0.5730\n",
            "Epoch 45/50\n",
            " - 39s - loss: 2.3444 - accuracy: 0.5628 - val_loss: 2.5220 - val_accuracy: 0.5505\n",
            "Epoch 46/50\n",
            " - 40s - loss: 2.1838 - accuracy: 0.5993 - val_loss: 2.3007 - val_accuracy: 0.5894\n",
            "Epoch 47/50\n",
            " - 39s - loss: 2.1232 - accuracy: 0.6111 - val_loss: 2.2000 - val_accuracy: 0.6071\n",
            "Epoch 48/50\n",
            " - 39s - loss: 2.0766 - accuracy: 0.6194 - val_loss: 2.2164 - val_accuracy: 0.6008\n",
            "Epoch 49/50\n",
            " - 39s - loss: 2.0578 - accuracy: 0.6246 - val_loss: 2.2710 - val_accuracy: 0.5979\n",
            "Epoch 50/50\n",
            " - 39s - loss: 2.0274 - accuracy: 0.6300 - val_loss: 2.3735 - val_accuracy: 0.5800\n",
            "Full validation loss:  0.42\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}