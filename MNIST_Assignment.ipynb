{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Assignment",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNpx1PjgkC9vS632Ug3l8bf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sruthi1996/DeepLearning/blob/master/MNIST_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2wyA4dPGKwx",
        "colab_type": "text"
      },
      "source": [
        "**MNIST classification using Numpy- Assignment-1**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-f057LYIBhD",
        "colab_type": "text"
      },
      "source": [
        "Importing the MNIST dataset using Keras; the handwritten digits will be used for classification using neural networks.\n",
        "Also, the Image data generator API from keras has been used for Image Augmentation.\n",
        "**Image Augmentation** is a technique where batches of image data will be generated. This creates artificial images using multiple ways like rotations, flips or shifts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR49gS9vIDQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KW80VZWIOyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download the MNIST data; dataset contains data for both training and testing\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOysTHVzIqtF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "effae6d1-4919-47f8-e935-a5bd37d045c1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(X_train[5], cmap=plt.get_cmap('gray'))\n",
        "plt.show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEjLHTeWcdQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy3HRmmIWtWn",
        "colab_type": "text"
      },
      "source": [
        "The Y labels or the target in both train and test data are digits ranging from 0 to 9, which have to be converted into vectors; "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsKP58NVJ1UF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "Y_train = np_utils.to_categorical(Y_train)\n",
        "Y_test = np_utils.to_categorical(Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_rWQYrbKOu6",
        "colab_type": "text"
      },
      "source": [
        "Now, to perform Normalization on the dataset; we are using **grayscale**. In **grayscale normalization**; the Min and Max values which are between 0 and 255 change to a new set of minimum and maximum values i.e. between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZY57MXiKUKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U85mdlnQKdes",
        "colab_type": "text"
      },
      "source": [
        "Now, we have to reshape the dataset into  the 784 pixel values i.e. into 3 dimensions; "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmXq-BeuKaWS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed72bd10-cb71-4ca2-f294-da71b8bee4e2"
      },
      "source": [
        "X_train = X_train.reshape(-1,28,28,1)\n",
        "print(X_train.shape)\n",
        "\n",
        "#X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "#X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7GSQbgBLP7W",
        "colab_type": "text"
      },
      "source": [
        "Similarily doing it for test data; "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHFC-0YjLOay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2cb8ac1-3589-4372-e919-01c9bf893bdc"
      },
      "source": [
        "X_test = X_test.reshape(-1,28,28,1)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nETEP-_LfgZ",
        "colab_type": "text"
      },
      "source": [
        "AS mentioned above; using an API from keras to perform image augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLD4r3VaLvyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datageneration = ImageDataGenerator(zoom_range=0.5, width_shift_range=0.05, height_shift_range=0.05)\n",
        "datageneration.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPSnksoWL8sq",
        "colab_type": "text"
      },
      "source": [
        "To perform training using neural networks, we have to define **foward and backward propagation functions**; also defining suitable **activation function**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9kbuoVZL7Oi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer_NN:\n",
        "    def __init__(self, in_neurons, out_neurons, dropout_value=0.3, learning_rate=0.1):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = np.random.randn(in_neurons, out_neurons) * 0.01\n",
        "        self.biases = np.zeros(out_neurons)\n",
        "\n",
        "    def forwardprop(self, input_activation):\n",
        "        activations = np.matmul(input_activation, self.weights) + self.biases\n",
        "        return activations\n",
        "\n",
        "    def backwardprop(self, input_activation, grad_output):\n",
        "        grad_input = np.dot(grad_output, np.transpose(self.weights))\n",
        "\n",
        "        # Gradient with Weights\n",
        "        grad_weights = np.transpose(np.dot(np.transpose(grad_output), input_activation))\n",
        "        # Gradient with Biases\n",
        "        grad_biases = np.sum(grad_output, axis=0)\n",
        "\n",
        "        # Stochastic Gradient Descent.\n",
        "        self.weights = self.weights - self.learning_rate * grad_weights\n",
        "        self.biases = self.biases - self.learning_rate * grad_biases\n",
        "        return grad_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGiNhZLFQgNq",
        "colab_type": "text"
      },
      "source": [
        "Defining **Rectified Linear Unit**(RELU function):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNABi6NaQmte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Relu:\n",
        "    @staticmethod\n",
        "    def forwardprop(input_activation):\n",
        "        return (input_activation > 0) * input_activation\n",
        "\n",
        "    @staticmethod\n",
        "    def backwardprop(input_activation, grad_output):\n",
        "        return (input_activation > 0) * grad_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trBekQz8Q6mI",
        "colab_type": "text"
      },
      "source": [
        "The **Relu function ranges the values with between 0 and any positive value.** We have used Relu activation function along with forward and backward propagation.\n",
        "Now, We have to define a **dropout layer** which nulls the activation value of certain neurons; The **drop_value** defined below is nothing but a **treshold** value used for dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7XnfcOcR0dJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dropout_fxn:\n",
        "    @staticmethod\n",
        "    def generate_dropout_mask(size, dropout_value):\n",
        "        zeros_size = int(size * dropout_value)\n",
        "        zeros = np.zeros(zeros_size)\n",
        "        ones = np.ones(size - zeros_size)\n",
        "        dropout_mask = np.asarray(zeros.tolist() + ones.tolist())\n",
        "        random.shuffle(dropout_mask)\n",
        "        return dropout_mask\n",
        "\n",
        "    def __init__(self, size, dropout_value=0.20):\n",
        "        self.dropout_mask = Dropout_fxn.generate_dropout_mask(size, dropout_value)\n",
        "\n",
        "    def forwardprop(self, input_activation):\n",
        "        return np.multiply(input_activation, self.dropout_mask)\n",
        "\n",
        "    def backwardprop(self, input_activation, grad_output):\n",
        "        return np.multiply(grad_output, self.dropout_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La2HYTlvSXVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neural_network = [\n",
        "    Layer_NN(X_train.shape[1] * X_train.shape[2], 200),\n",
        "    Dropout_fxn(200, 0.20),\n",
        "    Relu(),\n",
        "    Layer_NN(200, 100),\n",
        "    Dropout_fxn(100, 0.10),\n",
        "    Relu(),\n",
        "    Layer_NN(100, 10)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWL_7T0hTjho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmaxerr(out_activations, y):\n",
        "    softmax = np.exp(out_activations) / np.exp(out_activations).sum(axis=-1, keepdims=True)\n",
        "    return softmax - y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoPriRh_Tuit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_activations(X):\n",
        "    X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
        "    activations = []\n",
        "    for level in range(len(neural_network)):\n",
        "        next_activations = neural_network[level].forwardprop(X)\n",
        "        activations.append(next_activations)\n",
        "        X = next_activations\n",
        "    return activations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru_7mipOT-uR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb4dc9aa-ba00-47e0-b986-6f673850709a"
      },
      "source": [
        "def trainfxn(X, y):\n",
        "    activations = get_activations(X)\n",
        "    output_activations = activations[-1]\n",
        "\n",
        "    loss = softmaxerr(output_activations, y)\n",
        "    for i in range(1, len(neural_network)):\n",
        "        network = neural_network[len(neural_network) - i]\n",
        "        loss = network.backwardprop(activations[len(neural_network) - i - 1], loss)\n",
        "\n",
        "      \n",
        "def prediction_NN(X):\n",
        "    out_activations = get_activations(X)[-1]\n",
        "    return out_activations.argmax(axis=-1)\n",
        "\n",
        "\n",
        "def train_datagen(datageneration, total_iterations):\n",
        "  for iteration in range(total_iterations):\n",
        "      minibatch_iterations = 70\n",
        "      for X_batch, y_batch in datageneration.flow(X_train, Y_train, batch_size=8):\n",
        "          trainfxn(X_batch.reshape(-1, X_batch.shape[1], X_batch.shape[2]), y_batch)\n",
        "          minibatch_iterations -= 1\n",
        "          if minibatch_iterations == 0:\n",
        "              break\n",
        "      print(\"***Iteration: {}***\".format(iteration))\n",
        "      train_result = [np.mean(prediction_NN(X_train) == Y_train.argmax(axis=-1))]\n",
        "      print(\"Train accuracy: {}%\".format(train_result[-1] * 100))\n",
        "      test_result = [np.mean(prediction_NN(X_test) == Y_test.argmax(axis=-1))]\n",
        "      print(\"Test accuracy: {}%\".format(test_result[-1] * 100))\n",
        "\n",
        "# Training the model with Augumented data generated\n",
        "train_datagen(datageneration, 110)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***Iteration: 0***\n",
            "Train accuracy: 9.871666666666666%\n",
            "Test accuracy: 9.8%\n",
            "***Iteration: 1***\n",
            "Train accuracy: 9.915000000000001%\n",
            "Test accuracy: 10.09%\n",
            "***Iteration: 2***\n",
            "Train accuracy: 17.625%\n",
            "Test accuracy: 18.3%\n",
            "***Iteration: 3***\n",
            "Train accuracy: 34.901666666666664%\n",
            "Test accuracy: 35.47%\n",
            "***Iteration: 4***\n",
            "Train accuracy: 38.975%\n",
            "Test accuracy: 39.71%\n",
            "***Iteration: 5***\n",
            "Train accuracy: 55.794999999999995%\n",
            "Test accuracy: 55.2%\n",
            "***Iteration: 6***\n",
            "Train accuracy: 56.779999999999994%\n",
            "Test accuracy: 57.54%\n",
            "***Iteration: 7***\n",
            "Train accuracy: 46.845%\n",
            "Test accuracy: 47.85%\n",
            "***Iteration: 8***\n",
            "Train accuracy: 57.95%\n",
            "Test accuracy: 58.3%\n",
            "***Iteration: 9***\n",
            "Train accuracy: 56.06166666666667%\n",
            "Test accuracy: 56.43%\n",
            "***Iteration: 10***\n",
            "Train accuracy: 25.106666666666666%\n",
            "Test accuracy: 25.990000000000002%\n",
            "***Iteration: 11***\n",
            "Train accuracy: 65.69333333333334%\n",
            "Test accuracy: 66.28%\n",
            "***Iteration: 12***\n",
            "Train accuracy: 65.03333333333333%\n",
            "Test accuracy: 65.0%\n",
            "***Iteration: 13***\n",
            "Train accuracy: 56.53666666666667%\n",
            "Test accuracy: 56.56%\n",
            "***Iteration: 14***\n",
            "Train accuracy: 67.06833333333333%\n",
            "Test accuracy: 67.5%\n",
            "***Iteration: 15***\n",
            "Train accuracy: 55.031666666666666%\n",
            "Test accuracy: 55.1%\n",
            "***Iteration: 16***\n",
            "Train accuracy: 67.93666666666667%\n",
            "Test accuracy: 67.89%\n",
            "***Iteration: 17***\n",
            "Train accuracy: 68.735%\n",
            "Test accuracy: 68.33%\n",
            "***Iteration: 18***\n",
            "Train accuracy: 65.56833333333333%\n",
            "Test accuracy: 66.77%\n",
            "***Iteration: 19***\n",
            "Train accuracy: 65.04166666666666%\n",
            "Test accuracy: 65.41%\n",
            "***Iteration: 20***\n",
            "Train accuracy: 71.74166666666667%\n",
            "Test accuracy: 72.2%\n",
            "***Iteration: 21***\n",
            "Train accuracy: 61.35%\n",
            "Test accuracy: 62.94%\n",
            "***Iteration: 22***\n",
            "Train accuracy: 68.97%\n",
            "Test accuracy: 69.19%\n",
            "***Iteration: 23***\n",
            "Train accuracy: 72.64833333333334%\n",
            "Test accuracy: 73.00999999999999%\n",
            "***Iteration: 24***\n",
            "Train accuracy: 67.22833333333334%\n",
            "Test accuracy: 68.85%\n",
            "***Iteration: 25***\n",
            "Train accuracy: 69.365%\n",
            "Test accuracy: 70.28999999999999%\n",
            "***Iteration: 26***\n",
            "Train accuracy: 63.98833333333334%\n",
            "Test accuracy: 65.44%\n",
            "***Iteration: 27***\n",
            "Train accuracy: 71.64166666666667%\n",
            "Test accuracy: 72.33000000000001%\n",
            "***Iteration: 28***\n",
            "Train accuracy: 68.24%\n",
            "Test accuracy: 68.42%\n",
            "***Iteration: 29***\n",
            "Train accuracy: 73.105%\n",
            "Test accuracy: 74.28%\n",
            "***Iteration: 30***\n",
            "Train accuracy: 68.595%\n",
            "Test accuracy: 69.66%\n",
            "***Iteration: 31***\n",
            "Train accuracy: 65.605%\n",
            "Test accuracy: 66.19%\n",
            "***Iteration: 32***\n",
            "Train accuracy: 75.44833333333332%\n",
            "Test accuracy: 76.09%\n",
            "***Iteration: 33***\n",
            "Train accuracy: 72.23333333333333%\n",
            "Test accuracy: 73.72%\n",
            "***Iteration: 34***\n",
            "Train accuracy: 77.13666666666667%\n",
            "Test accuracy: 78.39%\n",
            "***Iteration: 35***\n",
            "Train accuracy: 76.97333333333334%\n",
            "Test accuracy: 77.25%\n",
            "***Iteration: 36***\n",
            "Train accuracy: 71.30166666666666%\n",
            "Test accuracy: 72.88%\n",
            "***Iteration: 37***\n",
            "Train accuracy: 75.93666666666667%\n",
            "Test accuracy: 76.46%\n",
            "***Iteration: 38***\n",
            "Train accuracy: 71.81166666666667%\n",
            "Test accuracy: 72.1%\n",
            "***Iteration: 39***\n",
            "Train accuracy: 74.83666666666666%\n",
            "Test accuracy: 75.25%\n",
            "***Iteration: 40***\n",
            "Train accuracy: 76.02166666666666%\n",
            "Test accuracy: 76.59%\n",
            "***Iteration: 41***\n",
            "Train accuracy: 75.31833333333333%\n",
            "Test accuracy: 76.05%\n",
            "***Iteration: 42***\n",
            "Train accuracy: 67.975%\n",
            "Test accuracy: 67.99%\n",
            "***Iteration: 43***\n",
            "Train accuracy: 69.505%\n",
            "Test accuracy: 70.30999999999999%\n",
            "***Iteration: 44***\n",
            "Train accuracy: 79.25166666666667%\n",
            "Test accuracy: 79.58%\n",
            "***Iteration: 45***\n",
            "Train accuracy: 77.52166666666666%\n",
            "Test accuracy: 78.42%\n",
            "***Iteration: 46***\n",
            "Train accuracy: 68.47%\n",
            "Test accuracy: 70.41%\n",
            "***Iteration: 47***\n",
            "Train accuracy: 76.61166666666666%\n",
            "Test accuracy: 78.16%\n",
            "***Iteration: 48***\n",
            "Train accuracy: 70.08333333333333%\n",
            "Test accuracy: 70.95%\n",
            "***Iteration: 49***\n",
            "Train accuracy: 75.325%\n",
            "Test accuracy: 75.78%\n",
            "***Iteration: 50***\n",
            "Train accuracy: 70.665%\n",
            "Test accuracy: 72.06%\n",
            "***Iteration: 51***\n",
            "Train accuracy: 70.65333333333334%\n",
            "Test accuracy: 71.89%\n",
            "***Iteration: 52***\n",
            "Train accuracy: 80.61166666666666%\n",
            "Test accuracy: 81.54%\n",
            "***Iteration: 53***\n",
            "Train accuracy: 72.94500000000001%\n",
            "Test accuracy: 73.0%\n",
            "***Iteration: 54***\n",
            "Train accuracy: 76.58166666666666%\n",
            "Test accuracy: 78.09%\n",
            "***Iteration: 55***\n",
            "Train accuracy: 74.845%\n",
            "Test accuracy: 75.75%\n",
            "***Iteration: 56***\n",
            "Train accuracy: 77.92%\n",
            "Test accuracy: 79.39%\n",
            "***Iteration: 57***\n",
            "Train accuracy: 78.075%\n",
            "Test accuracy: 78.64999999999999%\n",
            "***Iteration: 58***\n",
            "Train accuracy: 79.465%\n",
            "Test accuracy: 80.03%\n",
            "***Iteration: 59***\n",
            "Train accuracy: 80.59666666666668%\n",
            "Test accuracy: 81.13%\n",
            "***Iteration: 60***\n",
            "Train accuracy: 78.18166666666667%\n",
            "Test accuracy: 78.66%\n",
            "***Iteration: 61***\n",
            "Train accuracy: 78.38333333333334%\n",
            "Test accuracy: 80.19%\n",
            "***Iteration: 62***\n",
            "Train accuracy: 76.07666666666667%\n",
            "Test accuracy: 77.10000000000001%\n",
            "***Iteration: 63***\n",
            "Train accuracy: 79.74166666666666%\n",
            "Test accuracy: 80.78%\n",
            "***Iteration: 64***\n",
            "Train accuracy: 78.425%\n",
            "Test accuracy: 79.62%\n",
            "***Iteration: 65***\n",
            "Train accuracy: 80.425%\n",
            "Test accuracy: 80.28999999999999%\n",
            "***Iteration: 66***\n",
            "Train accuracy: 81.74833333333333%\n",
            "Test accuracy: 82.50999999999999%\n",
            "***Iteration: 67***\n",
            "Train accuracy: 72.845%\n",
            "Test accuracy: 73.56%\n",
            "***Iteration: 68***\n",
            "Train accuracy: 75.76833333333333%\n",
            "Test accuracy: 76.69%\n",
            "***Iteration: 69***\n",
            "Train accuracy: 78.52166666666666%\n",
            "Test accuracy: 79.35%\n",
            "***Iteration: 70***\n",
            "Train accuracy: 79.61500000000001%\n",
            "Test accuracy: 80.53%\n",
            "***Iteration: 71***\n",
            "Train accuracy: 79.22666666666667%\n",
            "Test accuracy: 79.58%\n",
            "***Iteration: 72***\n",
            "Train accuracy: 79.73833333333333%\n",
            "Test accuracy: 80.77%\n",
            "***Iteration: 73***\n",
            "Train accuracy: 79.985%\n",
            "Test accuracy: 80.78999999999999%\n",
            "***Iteration: 74***\n",
            "Train accuracy: 79.98166666666665%\n",
            "Test accuracy: 80.58%\n",
            "***Iteration: 75***\n",
            "Train accuracy: 77.7%\n",
            "Test accuracy: 78.16%\n",
            "***Iteration: 76***\n",
            "Train accuracy: 80.25166666666667%\n",
            "Test accuracy: 80.71000000000001%\n",
            "***Iteration: 77***\n",
            "Train accuracy: 81.34333333333333%\n",
            "Test accuracy: 82.06%\n",
            "***Iteration: 78***\n",
            "Train accuracy: 80.17666666666666%\n",
            "Test accuracy: 81.02000000000001%\n",
            "***Iteration: 79***\n",
            "Train accuracy: 80.325%\n",
            "Test accuracy: 81.41000000000001%\n",
            "***Iteration: 80***\n",
            "Train accuracy: 81.39833333333333%\n",
            "Test accuracy: 81.53%\n",
            "***Iteration: 81***\n",
            "Train accuracy: 81.60833333333333%\n",
            "Test accuracy: 82.43%\n",
            "***Iteration: 82***\n",
            "Train accuracy: 79.38499999999999%\n",
            "Test accuracy: 79.95%\n",
            "***Iteration: 83***\n",
            "Train accuracy: 79.03833333333333%\n",
            "Test accuracy: 80.43%\n",
            "***Iteration: 84***\n",
            "Train accuracy: 82.38666666666667%\n",
            "Test accuracy: 83.05%\n",
            "***Iteration: 85***\n",
            "Train accuracy: 77.03999999999999%\n",
            "Test accuracy: 77.41%\n",
            "***Iteration: 86***\n",
            "Train accuracy: 81.85666666666667%\n",
            "Test accuracy: 82.39%\n",
            "***Iteration: 87***\n",
            "Train accuracy: 81.55%\n",
            "Test accuracy: 82.49%\n",
            "***Iteration: 88***\n",
            "Train accuracy: 79.81166666666667%\n",
            "Test accuracy: 80.12%\n",
            "***Iteration: 89***\n",
            "Train accuracy: 77.30166666666666%\n",
            "Test accuracy: 78.36999999999999%\n",
            "***Iteration: 90***\n",
            "Train accuracy: 75.85499999999999%\n",
            "Test accuracy: 77.03999999999999%\n",
            "***Iteration: 91***\n",
            "Train accuracy: 79.835%\n",
            "Test accuracy: 80.84%\n",
            "***Iteration: 92***\n",
            "Train accuracy: 79.89833333333334%\n",
            "Test accuracy: 80.67%\n",
            "***Iteration: 93***\n",
            "Train accuracy: 82.23833333333333%\n",
            "Test accuracy: 82.85%\n",
            "***Iteration: 94***\n",
            "Train accuracy: 83.0%\n",
            "Test accuracy: 83.46000000000001%\n",
            "***Iteration: 95***\n",
            "Train accuracy: 76.93833333333333%\n",
            "Test accuracy: 78.2%\n",
            "***Iteration: 96***\n",
            "Train accuracy: 83.43166666666667%\n",
            "Test accuracy: 84.42%\n",
            "***Iteration: 97***\n",
            "Train accuracy: 78.84166666666667%\n",
            "Test accuracy: 79.57%\n",
            "***Iteration: 98***\n",
            "Train accuracy: 80.46666666666667%\n",
            "Test accuracy: 81.05%\n",
            "***Iteration: 99***\n",
            "Train accuracy: 81.34166666666667%\n",
            "Test accuracy: 82.49%\n",
            "***Iteration: 100***\n",
            "Train accuracy: 78.82333333333334%\n",
            "Test accuracy: 79.58%\n",
            "***Iteration: 101***\n",
            "Train accuracy: 80.66166666666666%\n",
            "Test accuracy: 80.88%\n",
            "***Iteration: 102***\n",
            "Train accuracy: 84.50333333333333%\n",
            "Test accuracy: 85.09%\n",
            "***Iteration: 103***\n",
            "Train accuracy: 82.61666666666667%\n",
            "Test accuracy: 83.25%\n",
            "***Iteration: 104***\n",
            "Train accuracy: 81.61833333333334%\n",
            "Test accuracy: 82.24000000000001%\n",
            "***Iteration: 105***\n",
            "Train accuracy: 82.055%\n",
            "Test accuracy: 81.96%\n",
            "***Iteration: 106***\n",
            "Train accuracy: 82.02000000000001%\n",
            "Test accuracy: 82.67999999999999%\n",
            "***Iteration: 107***\n",
            "Train accuracy: 81.605%\n",
            "Test accuracy: 82.43%\n",
            "***Iteration: 108***\n",
            "Train accuracy: 82.79833333333333%\n",
            "Test accuracy: 82.94%\n",
            "***Iteration: 109***\n",
            "Train accuracy: 80.68166666666666%\n",
            "Test accuracy: 81.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY1DOTmVaLtL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "9072ab68-007c-4730-b12b-6716212340a6"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred = prediction_NN(X_test)\n",
        "print('confusion matrix on test set:\\n', confusion_matrix(Y_test.argmax(axis=-1), y_pred))\n",
        "final_test_accuracy = np.mean(prediction_NN(X_test) == Y_test.argmax(axis=-1)) * 100\n",
        "print(\"Final Test Accuracy: {}%\".format(final_test_accuracy))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion matrix on test set:\n",
            " [[ 868    2    2    3   17   30    2    9   30   17]\n",
            " [   0 1116    1    4    0    1    2    0   11    0]\n",
            " [   2   17  763   30   14    5   26   33  136    6]\n",
            " [   0    3    5  840    3   53    1    3   95    7]\n",
            " [   1    1    2    1  825    7   34    0   67   44]\n",
            " [   7    7    2   72    6  730    7    2   56    3]\n",
            " [   9    2    2    3   72   34  756    1   78    1]\n",
            " [   4   19   18   27    6    9    1  808   54   82]\n",
            " [   2   10    0   15    9   19    4    2  903   10]\n",
            " [   2    3    6   30  219   19    3   14  161  552]]\n",
            "Final Test Accuracy: 81.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZutqUh6ba3r",
        "colab_type": "text"
      },
      "source": [
        "Hence, with a 3 layered network; the accuracy has been 81.6% on the test data."
      ]
    }
  ]
}
