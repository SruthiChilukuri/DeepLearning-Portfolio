{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCaptioning_tensorflow",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q293StXllNtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "#from nmt_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDAK69WZnAkC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "55b9891f-6832-4f6c-b962-3c8d9463e236"
      },
      "source": [
        "annotation_folder = '/annotations/'\n",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
        "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
        "                                          cache_subdir=os.path.abspath('.'),\n",
        "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                          extract = True)\n",
        "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "  os.remove(annotation_zip)\n",
        "\n",
        "# Download image files\n",
        "image_folder = '/train2014/'\n",
        "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
        "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract = True)\n",
        "  PATH = os.path.dirname(image_zip) + image_folder\n",
        "  os.remove(image_zip)\n",
        "else:\n",
        "  PATH = os.path.abspath('.') + image_folder"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
            "13510574080/13510573713 [==============================] - 316s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2x0wwpiniHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "\n",
        "all_captions = []\n",
        "all_img_name_vector = []\n",
        "\n",
        "for annot in annotations['annotations']:\n",
        "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
        "    image_id = annot['image_id']\n",
        "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
        "\n",
        "    all_img_name_vector.append(full_coco_image_path)\n",
        "    all_captions.append(caption)\n",
        "\n",
        "train_captions, img_name_vector = shuffle(all_captions,\n",
        "                                          all_img_name_vector,\n",
        "                                          random_state=1)\n",
        "\n",
        "num_examples = 10000\n",
        "train_captions = train_captions[:num_examples]\n",
        "img_name_vector = img_name_vector[:num_examples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc6GIcCBnrPD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3ffa8d3-384d-4440-e062-488168b036a4"
      },
      "source": [
        "len(train_captions), len(all_captions)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 414113)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBpwYZMknvkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZYnHgC3nxcM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "97818713-12d4-4f99-cba3-91b43ccfc2c3"
      },
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmF8cg07n0Cs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encode_train = sorted(set(img_name_vector))\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "\n",
        "for img, path in image_dataset:\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARtLIfDsn4g8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DJ9SyS0n7fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_k = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viIyBFqfn9fJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6Oo6DskoAPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
        "max_length = calc_max_length(train_seqs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7BqeO6HoEIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
        "                                                                    cap_vector,\n",
        "                                                                    test_size=0.2,\n",
        "                                                                    random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB_8K4ku2JGZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd6acdea-431f-49ea-88fd-e7a5d0dece71"
      },
      "source": [
        "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 8000, 2000, 2000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpIgV488oI5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = top_k + 1\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMqx_zcuoMLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "  return img_tensor, cap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UeCXhHjoPQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpSXvql1oSIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nE0fIGwohnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgQFuaxBokjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    \n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    output, state = self.gru(x)\n",
        "    x = self.fc1(output)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "    x = self.fc2(x)\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYgbK9Tqo2fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ErSuC2qo4uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ji1HqO5o7NR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTEkY0wEo9HX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "  # restoring the latest checkpoint in checkpoint_path\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVe4H5nNo9LL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "  loss = 0\n",
        "\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      features = encoder(img_tensor)\n",
        "\n",
        "      for i in range(1, target.shape[1]):\n",
        "          # passing the features through the decoder\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "  return loss, total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39JMV7PBpEd3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "f9b2caba-b1b7-4d3e-9113-16e32dc7a6aa"
      },
      "source": [
        "EPOCHS = 2\n",
        "loss_plot = []\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
        "                                         total_loss/num_steps))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.0939\n",
            "Epoch 1 Batch 100 Loss 0.9537\n",
            "Epoch 1 Loss 1.001078\n",
            "Time taken for 1 epoch 54.96562743186951 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.9874\n",
            "Epoch 2 Batch 100 Loss 0.8432\n",
            "Epoch 2 Loss 0.865289\n",
            "Time taken for 1 epoch 49.502859115600586 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNhk7QmqpGwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e16d1448-07c5-4e99-a9f6-2c6011a075a4"
      },
      "source": [
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}